"""
ë©€í‹° ì—ì´ì „íŠ¸ í™˜ê²½ í´ë˜ìŠ¤.

4ë©´ ë²½ìœ¼ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ 2D í‰ë©´ì—ì„œ ì—ì´ì „íŠ¸ë“¤ì´ ëª©ì ì§€ë¡œ ì´ë™.
"""

from typing import List, Tuple, Dict, Any, Optional
import numpy as np

from .dynamics import AgentState, Dynamics

try:
    from ..config import EnvConfig, RobotConfig
except ImportError:
    from config import EnvConfig, RobotConfig


class MultiAgentEnv:
    """ë©€í‹° ì—ì´ì „íŠ¸ 2D í™˜ê²½."""

    def __init__(self, env_config: EnvConfig, robot_config: RobotConfig):
        """
        Args:
            env_config: í™˜ê²½ ì„¤ì •
            robot_config: ë¡œë´‡ ì„¤ì •
        """
        self.env_config = env_config
        self.robot_config = robot_config

        # ë¬¼ë¦¬ ì—”ì§„
        self.dynamics = Dynamics(robot_config)

        # ìƒíƒœ ë³€ìˆ˜
        self.states: List[AgentState] = []
        self.goals: np.ndarray = np.zeros((env_config.num_agents, 2))
        self.step_count: int = 0
        self.prev_distances: np.ndarray = np.zeros(env_config.num_agents)
        self.arrived: np.ndarray = np.zeros(env_config.num_agents, dtype=bool)

    def reset(self, seed: Optional[int] = None) -> Dict[str, Any]:
        """
        í™˜ê²½ ì´ˆê¸°í™”. ì—ì´ì „íŠ¸ ìœ„ì¹˜ì™€ ëª©ì ì§€ ìƒì„±.

        Args:
            seed: ëœë¤ ì‹œë“œ (ì„ íƒ)

        Returns:
            dict with keys:
                - states: List[AgentState] - ì´ˆê¸° ì—ì´ì „íŠ¸ ìƒíƒœë“¤
                - goals: np.ndarray shape (N, 2) - ëª©ì ì§€ ì¢Œí‘œ
        """
        if seed is not None:
            np.random.seed(seed)

        self.step_count = 0
        self.arrived = np.zeros(self.env_config.num_agents, dtype=bool)

        # ì—ì´ì „íŠ¸ ì´ˆê¸° ìœ„ì¹˜ ìƒì„± (ì„œë¡œ ì¶©ëŒ ì—†ì´)
        self.states = self._generate_initial_positions()

        # êµì°¨í•˜ëŠ” ëª©ì ì§€ ìƒì„±
        self.goals = self._generate_crossing_goals()

        # ì´ˆê¸° ê±°ë¦¬ ì €ì¥ (ë³´ìƒ ê³„ì‚°ìš©)
        self.prev_distances = np.array([
            np.linalg.norm(self.states[i].position - self.goals[i])
            for i in range(self.env_config.num_agents)
        ])

        return {
            'states': self.states,
            'goals': self.goals.copy()
        }

    def step(self, actions: np.ndarray) -> Tuple[Dict[str, Any], np.ndarray, bool, Dict[str, Any]]:
        """
        í•œ ìŠ¤í… ì§„í–‰.

        Args:
            actions: shape (N, 2) - ê° ì—ì´ì „íŠ¸ì˜ í˜ ì…ë ¥

        Returns:
            tuple of:
                - obs: dict - ìƒˆë¡œìš´ observation
                - rewards: np.ndarray shape (N,) - ê° ì—ì´ì „íŠ¸ ë³´ìƒ
                - done: bool - ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
                - info: dict - ì¶”ê°€ ì •ë³´ (ì¶©ëŒ ì—¬ë¶€, ë„ì°© ì—¬ë¶€ ë“±)
        """
        actions = np.asarray(actions, dtype=np.float64)
        assert actions.shape == (self.env_config.num_agents, 2)

        self.step_count += 1
        collisions = []
        rewards = np.zeros(self.env_config.num_agents)

        # ê° ì—ì´ì „íŠ¸ì— ëŒ€í•´ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        new_states = []
        for i in range(self.env_config.num_agents):
            # Dynamics ì ìš©
            new_state = self.dynamics.step(
                self.states[i],
                actions[i],
                self.env_config.dt
            )

            # Hard constraint: ë²½ ì¶©ëŒ ì²˜ë¦¬ (í˜„ì‹¤ì  - íŠ•ê¹€ ì—†ìŒ)
            new_state = self._apply_wall_constraint(new_state)

            new_states.append(new_state)

        self.states = new_states

        # ì—ì´ì „íŠ¸ ê°„ ì¶©ëŒ ê²€ì‚¬ (ë¬¼ë¦¬ ì œì•½ ì ìš© ì „ì— ê²€ì‚¬í•´ì•¼ í•¨!)
        collisions = self._check_collision()

        # ì—ì´ì „íŠ¸ ê°„ ì¶©ëŒ ì œì•½ (Hard constraint - ë²½ì²˜ëŸ¼)
        self._apply_agent_collision_constraint()

        # ë³´ìƒ ê³„ì‚°
        for i in range(self.env_config.num_agents):
            rewards[i] = self._compute_reward(i, collisions)

        # ë„ì°© ì—¬ë¶€ ì—…ë°ì´íŠ¸
        for i in range(self.env_config.num_agents):
            dist = np.linalg.norm(self.states[i].position - self.goals[i])
            if dist < self.env_config.goal_threshold:
                self.arrived[i] = True

        # í˜„ì¬ ê±°ë¦¬ ì €ì¥ (ë‹¤ìŒ ìŠ¤í… ë³´ìƒ ê³„ì‚°ìš©)
        self.prev_distances = np.array([
            np.linalg.norm(self.states[i].position - self.goals[i])
            for i in range(self.env_config.num_agents)
        ])

        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        done = self._is_done()

        obs = {
            'states': self.states,
            'goals': self.goals.copy()
        }

        info = {
            'collisions': collisions,
            'arrived': self.arrived.copy(),
            'step_count': self.step_count
        }

        return obs, rewards, done, info

    def _generate_initial_positions(self) -> List[AgentState]:
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸° ìœ„ì¹˜ ìƒì„± (ì„œë¡œ ì¶©ëŒ ì—†ì´).

        Returns:
            ì´ˆê¸° ì—ì´ì „íŠ¸ ìƒíƒœ ë¦¬ìŠ¤íŠ¸
        """
        states = []
        min_distance = 2.5 * self.robot_config.radius  # ìµœì†Œ ê±°ë¦¬
        margin = self.robot_config.radius + 0.1  # ë²½ìœ¼ë¡œë¶€í„° ì—¬ìœ 

        for _ in range(self.env_config.num_agents):
            max_attempts = 100
            for attempt in range(max_attempts):
                # ëœë¤ ìœ„ì¹˜ ìƒì„±
                x = np.random.uniform(margin, self.env_config.width - margin)
                y = np.random.uniform(margin, self.env_config.height - margin)
                position = np.array([x, y])

                # ê¸°ì¡´ ì—ì´ì „íŠ¸ì™€ ì¶©ëŒ ê²€ì‚¬
                valid = True
                for existing in states:
                    dist = np.linalg.norm(position - existing.position)
                    if dist < min_distance:
                        valid = False
                        break

                if valid:
                    states.append(AgentState(position=position, velocity=np.zeros(2)))
                    break

            if len(states) < _ + 1:
                # ì‹¤íŒ¨ ì‹œ ê°•ì œ ë°°ì¹˜
                x = margin + (_ % 2) * (self.env_config.width - 2 * margin)
                y = margin + (_ // 2) * (self.env_config.height - 2 * margin)
                states.append(AgentState(position=np.array([x, y]), velocity=np.zeros(2)))

        return states

    def _generate_crossing_goals(self) -> np.ndarray:
        """
        ì—ì´ì „íŠ¸ë“¤ì´ êµì°¨í•˜ë„ë¡ ëª©ì ì§€ ìƒì„±.
        ëŒ€ê°ì„  ìŠ¤ì™‘ ë°©ì‹: ì—ì´ì „íŠ¸ë¥¼ 4ë¶„ë©´ì— ë°°ì¹˜ í›„ ëŒ€ê°ì„  ë°˜ëŒ€í¸ì„ ëª©ì ì§€ë¡œ.

        Returns:
            ëª©ì ì§€ ì¢Œí‘œ (N, 2)
        """
        goals = np.zeros((self.env_config.num_agents, 2))
        margin = self.robot_config.radius + 0.2

        if self.env_config.num_agents == 4:
            # 4ë¶„ë©´ ëŒ€ê°ì„  êµì°¨
            for i in range(4):
                # í˜„ì¬ ì—ì´ì „íŠ¸ ìœ„ì¹˜ì˜ ëŒ€ê°ì„  ë°˜ëŒ€í¸
                current_pos = self.states[i].position
                center = np.array([self.env_config.width / 2, self.env_config.height / 2])

                # ì¤‘ì‹¬ ê¸°ì¤€ ë°˜ëŒ€í¸ìœ¼ë¡œ ëª©ì ì§€ ì„¤ì •
                offset = current_pos - center
                goal = center - offset

                # ë²”ìœ„ ë‚´ë¡œ í´ë¦¬í•‘
                goal[0] = np.clip(goal[0], margin, self.env_config.width - margin)
                goal[1] = np.clip(goal[1], margin, self.env_config.height - margin)

                goals[i] = goal
        else:
            # ì¼ë°˜ì ì¸ ê²½ìš°: ëœë¤ ëª©ì ì§€ (êµì°¨ ìœ ë„)
            min_goal_dist = 2.5 * self.robot_config.radius  # ëª©í‘œ ê°„ ìµœì†Œ ê±°ë¦¬

            for i in range(self.env_config.num_agents):
                max_attempts = 100
                for attempt in range(max_attempts):
                    x = np.random.uniform(margin, self.env_config.width - margin)
                    y = np.random.uniform(margin, self.env_config.height - margin)
                    goal = np.array([x, y])

                    # ìê¸° ìœ„ì¹˜ì™€ ì¶©ë¶„íˆ ë©€ì–´ì•¼ í•¨
                    if np.linalg.norm(goal - self.states[i].position) < self.env_config.width / 4:
                        continue

                    # ê¸°ì¡´ ëª©í‘œë“¤ê³¼ ê²¹ì¹˜ì§€ ì•Šì•„ì•¼ í•¨
                    valid = True
                    for j in range(i):
                        if np.linalg.norm(goal - goals[j]) < min_goal_dist:
                            valid = False
                            break

                    if valid:
                        goals[i] = goal
                        break

                # ì‹¤íŒ¨ ì‹œ ê·¸ëƒ¥ ë°°ì¹˜
                if attempt == max_attempts - 1:
                    goals[i] = goal

        return goals

    def _check_collision(self) -> List[Tuple[int, int]]:
        """
        ì—ì´ì „íŠ¸ ê°„ ê·¼ì ‘ ê²€ì‚¬ (í˜ë„í‹°ìš©).

        ë¬¼ë¦¬ì  ì¶©ëŒ(ê²¹ì¹¨)ì€ _apply_agent_collision_constraintì—ì„œ ì²˜ë¦¬.
        ì—¬ê¸°ì„œëŠ” ê·¼ì ‘ ì‹œ í˜ë„í‹°ë¥¼ ì£¼ê¸° ìœ„í•œ ê²€ì‚¬.

        Returns:
            ê·¼ì ‘í•œ ì—ì´ì „íŠ¸ ìŒì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        collisions = []
        # ë¬¼ë¦¬ì  ì ‘ì´‰ ì‹œì—ë§Œ í˜ë„í‹° (ë§ˆì§„ ì—†ìŒ)
        # ë¬¼ë¦¬ ì¶©ëŒ ì œì•½ìœ¼ë¡œ ê²¹ì¹¨ì€ ë°©ì§€ë˜ì§€ë§Œ, ì ‘ì´‰ ì‹œë„ ìì²´ì— í˜ë„í‹°
        proximity_dist = 2 * self.robot_config.radius

        for i in range(self.env_config.num_agents):
            for j in range(i + 1, self.env_config.num_agents):
                dist = np.linalg.norm(
                    self.states[i].position - self.states[j].position
                )
                if dist < proximity_dist:
                    collisions.append((i, j))

        return collisions

    def _apply_wall_constraint(self, state: AgentState) -> AgentState:
        """
        ë²½ ì¶©ëŒ ì‹œ ìœ„ì¹˜ í´ë¦¬í•‘ ë° ì†ë„ ì œê±° (Hard constraint).

        Args:
            state: ì—ì´ì „íŠ¸ ìƒíƒœ

        Returns:
            ìˆ˜ì •ëœ ì—ì´ì „íŠ¸ ìƒíƒœ
        """
        pos = state.position.copy()
        vel = state.velocity.copy()
        radius = self.robot_config.radius

        # ì™¼ìª½ ë²½
        if pos[0] < radius:
            pos[0] = radius
            vel[0] = max(vel[0], 0)  # ë²½ ë°©í–¥ ì†ë„ë§Œ ì œê±°

        # ì˜¤ë¥¸ìª½ ë²½
        if pos[0] > self.env_config.width - radius:
            pos[0] = self.env_config.width - radius
            vel[0] = min(vel[0], 0)

        # ì•„ë˜ìª½ ë²½
        if pos[1] < radius:
            pos[1] = radius
            vel[1] = max(vel[1], 0)

        # ìœ„ìª½ ë²½
        if pos[1] > self.env_config.height - radius:
            pos[1] = self.env_config.height - radius
            vel[1] = min(vel[1], 0)

        return AgentState(position=pos, velocity=vel)

    def _apply_agent_collision_constraint(self):
        """
        ì—ì´ì „íŠ¸ ê°„ ì¶©ëŒ ì‹œ ìœ„ì¹˜ ë¶„ë¦¬ ë° ì†ë„ ì¡°ì • (Hard constraint).

        ë²½ì²˜ëŸ¼ ì„œë¡œ í†µê³¼í•˜ì§€ ëª»í•˜ë„ë¡ í•¨.
        ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ ë™ì‹œì— ê²¹ì¹  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì²˜ë¦¬.
        """
        radius = self.robot_config.radius
        min_dist = 2 * radius  # ë‘ ì—ì´ì „íŠ¸ ì¤‘ì‹¬ ê°„ ìµœì†Œ ê±°ë¦¬

        # ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ ëª¨ë“  ì¶©ëŒ í•´ê²° (ìµœëŒ€ 10íšŒ)
        for _ in range(10):
            resolved = True

            for i in range(self.env_config.num_agents):
                for j in range(i + 1, self.env_config.num_agents):
                    pos_i = self.states[i].position
                    pos_j = self.states[j].position
                    vel_i = self.states[i].velocity
                    vel_j = self.states[j].velocity

                    # ë‘ ì—ì´ì „íŠ¸ ê°„ ê±°ë¦¬
                    diff = pos_j - pos_i
                    dist = np.linalg.norm(diff)

                    if dist < min_dist and dist > 1e-6:
                        resolved = False

                        # ê²¹ì¹¨ëŸ‰
                        overlap = min_dist - dist

                        # ë¶„ë¦¬ ë°©í–¥ (iì—ì„œ jë¡œ)
                        direction = diff / dist

                        # ê°ê° ì ˆë°˜ì”© ë°€ì–´ë‚´ê¸°
                        separation = direction * (overlap / 2 + 1e-4)
                        new_pos_i = pos_i - separation
                        new_pos_j = pos_j + separation

                        # ì†ë„ ì¡°ì •: ì„œë¡œë¥¼ í–¥í•˜ëŠ” ì†ë„ ì„±ë¶„ ì œê±°
                        vel_along_i = np.dot(vel_i, direction)
                        vel_along_j = np.dot(vel_j, -direction)

                        new_vel_i = vel_i.copy()
                        new_vel_j = vel_j.copy()

                        # iê°€ j ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë©´ ê·¸ ì„±ë¶„ ì œê±°
                        if vel_along_i > 0:
                            new_vel_i = vel_i - vel_along_i * direction

                        # jê°€ i ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë©´ ê·¸ ì„±ë¶„ ì œê±°
                        if vel_along_j > 0:
                            new_vel_j = vel_j - vel_along_j * (-direction)

                        # ìƒíƒœ ì—…ë°ì´íŠ¸
                        self.states[i] = AgentState(position=new_pos_i, velocity=new_vel_i)
                        self.states[j] = AgentState(position=new_pos_j, velocity=new_vel_j)

                    elif dist <= 1e-6:
                        # ì™„ì „íˆ ê²¹ì¹œ ê²½ìš°: ëœë¤ ë°©í–¥ìœ¼ë¡œ ë¶„ë¦¬
                        resolved = False
                        direction = np.random.randn(2)
                        direction = direction / (np.linalg.norm(direction) + 1e-8)

                        separation = direction * (min_dist / 2 + 1e-4)
                        new_pos_i = pos_i - separation
                        new_pos_j = pos_j + separation

                        self.states[i] = AgentState(position=new_pos_i, velocity=np.zeros(2))
                        self.states[j] = AgentState(position=new_pos_j, velocity=np.zeros(2))

            # ë¶„ë¦¬ í›„ ë²½ ì œì•½ ë‹¤ì‹œ ì ìš©
            for i in range(self.env_config.num_agents):
                self.states[i] = self._apply_wall_constraint(self.states[i])

            if resolved:
                break

    def _compute_reward(self, agent_idx: int, collisions: List[Tuple[int, int]]) -> float:
        """
        ë‹¨ì¼ ì—ì´ì „íŠ¸ì˜ ë³´ìƒ ê³„ì‚°.
        r_t = Î·â‚ Î”Î¼áµ¢ - Î·â‚‚ Î£â±¼ ğŸ™[ráµ¢â±¼ < r_safe] + Î·â‚ƒ ğŸ™[Î¼áµ¢ = 1]
        Args:
            agent_idx: ì—ì´ì „íŠ¸ ì¸ë±ìŠ¤
            collisions: ì¶©ëŒ ìŒ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³´ìƒ ê°’
        """
        reward = 0.0
        pos = self.states[agent_idx].position
        current_dist = np.linalg.norm(pos - self.goals[agent_idx])

        # 1. ì—ì´ì „íŠ¸ ì¶©ëŒ í˜ë„í‹°
        for (i, j) in collisions:
            if agent_idx == i or agent_idx == j:
                reward += self.env_config.collision_penalty  # -10.0
                break

        # 1b. ê·¼ì ‘ í˜ë„í‹° (ì¶©ëŒ ì „ì—ë„ ê°€ê¹Œìš°ë©´ í˜ë„í‹°)
        proximity_threshold = 3.0 * self.robot_config.radius  # ~0.315m
        collision_dist = 2.0 * self.robot_config.radius  # ~0.21m
        for i in range(self.env_config.num_agents):
            if i != agent_idx:
                dist = np.linalg.norm(pos - self.states[i].position)
                if dist < proximity_threshold and dist > collision_dist:
                    # ê°€ê¹Œìš¸ìˆ˜ë¡ í˜ë„í‹° ì¦ê°€ (ìµœëŒ€ -0.5)
                    proximity_ratio = 1.0 - (dist - collision_dist) / (proximity_threshold - collision_dist)
                    reward -= 0.5 * proximity_ratio

        # 2. ë²½ ì¶©ëŒ í˜ë„í‹°
        wall_contact_dist = self.robot_config.radius + 0.01
        if (pos[0] < wall_contact_dist or
            pos[0] > self.env_config.width - wall_contact_dist or
            pos[1] < wall_contact_dist or
            pos[1] > self.env_config.height - wall_contact_dist):
            reward -= 5.0

        # 3. ë„ì°© ë³´ë„ˆìŠ¤ (ìµœì´ˆ ë„ì°© ì‹œ 1íšŒ)
        if current_dist < self.env_config.goal_threshold and not self.arrived[agent_idx]:
            reward += self.env_config.goal_reward  # +10.0

        # 4. Goal ìœ ì§€ ë³´ìƒ (ë„ì°© í›„ ë§¤ ìŠ¤í…)
        if current_dist < self.env_config.goal_threshold:
            reward += 1.0  # ë§¤ ìŠ¤í… +1.0

        # 5. ë„ì°© í›„ ì´íƒˆ í˜ë„í‹°
        if self.arrived[agent_idx]:
            if current_dist >= self.env_config.goal_threshold:
                reward -= 5.0

        return reward

    def _is_done(self) -> bool:
        """
        ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´ í™•ì¸.

        Returns:
            ì¢…ë£Œ ì—¬ë¶€
        """
        # ëª¨ë“  ì—ì´ì „íŠ¸ ë„ì°©
        if np.all(self.arrived):
            return True

        # max_steps ë„ë‹¬
        if self.step_count >= self.env_config.max_steps:
            return True

        return False

    def get_observation(self, agent_idx: int) -> np.ndarray:
        """
        ë‹¨ì¼ ì—ì´ì „íŠ¸ ê´€ì ì˜ observation ìƒì„±.

        Args:
            agent_idx: ì—ì´ì „íŠ¸ ì¸ë±ìŠ¤

        Returns:
            observation ë²¡í„° [ìê¸° ìƒíƒœ, ëª©í‘œ ì˜¤í”„ì…‹, ì´ì›ƒ ìƒíƒœë“¤]
        """
        state = self.states[agent_idx]
        goal = self.goals[agent_idx]

        # ìê¸° ìƒíƒœ: [x, y, vx, vy]
        self_state = np.concatenate([state.position, state.velocity])

        # ëª©í‘œ ì˜¤í”„ì…‹: [dx, dy]
        goal_offset = goal - state.position

        # ì´ì›ƒ ìƒíƒœë“¤: [(x, y, vx, vy), ...]
        neighbor_states = []
        for i in range(self.env_config.num_agents):
            if i != agent_idx:
                neighbor = self.states[i]
                # ìƒëŒ€ì  ìœ„ì¹˜ì™€ ì†ë„
                rel_pos = neighbor.position - state.position
                rel_vel = neighbor.velocity - state.velocity
                neighbor_states.extend([*rel_pos, *rel_vel])

        return np.concatenate([self_state, goal_offset, neighbor_states])
